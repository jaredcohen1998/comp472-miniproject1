1) An analysis of the initial dataset given on Moodle. If there is anything particular about these datasets
that might have an impact on the performance of some models, explain it.

A: For the datasets, having a large variation in the amount of articles per category can have an impact on performance. We want a large enough dataset per category to train our model, with each class having roughly the same amount of data seen. Luckily, this is a non issue in the bbc dataset (as seen in bbc-distribution.pdf). This is however an issue in the drug dataset, where drugY is seen much more than the others. This can have an impact on performance on our model.



2) An analysis of the results of all the models with the data sets. In particular, compare and contrast the
performance of each model with one another, and with the datasets. Please note that your presentation
must be analytical. This means that in addition to stating the facts (e.g. the macro-F1 has this value),
you should also analyse them (i.e. explain why some metric seems more appropriate than another, or
why your model did not do as well as expected. Tables, graphs and contingency tables to back up
your claims would be very welcome here.

A: In the BBC dataset, smoothed and unsmoothed Naive Bayes classifiers showed exceptionally high precision and recall values. Entertainment, which had the least number of articles in the dataset, scored 1.00 consistently in precision and 0.99 in the F1 score. This higher precision could be attributed to the low sample size which may have led to overfitting the test data. This works in its favour in this scenario where the test size is only 20%, but might be less effective for larger test sizes. 

For the drug dataset, the data was much more skewed compared to the BBC set. Drugs A, B, and C were only sizes 23, 16, and 16 respectively, in contrast to DrugY, which had the highest sample size of 91. This skew in the dataset reflects in some of the models. Perceptron and Top Decision Tree classifiers had such horrible results classifying Drugs A, B, and C that they couldn't accurately classify any of the test data for those targets (as shown in their confusion matrices), which lead to a 0.0 value for both recall and precision. In contrast, the Base Decision Tree model and Guassian Naive Bayes model scored perfectly in precision and recall, which means they also scored perfect F1 averages and accuracy.


3) In the case of team work, a description of the responsibilities and contributions of each team member

A: We all worked on it an equal amount for both task 1 and task 2. We should hop in voice/video calls and all contribute together. In these calls, one person would share their screen and actually write the code to push to Github.